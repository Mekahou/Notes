{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b0cd6e-0583-4a86-b7ae-600d9c70fe3e",
   "metadata": {},
   "source": [
    "**From a series of application of modern interpolation methods for economics: written by [Mahdi E Kahou](https://sites.google.com/site/mahdiebrahimikahou/about-me)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a4b7b-cb7f-409e-80a8-6f664c8e25e8",
   "metadata": {},
   "source": [
    "# Goal of this notebook  \n",
    "\n",
    "The purpose of this notebook is to demonstrate:  \n",
    "1. How to numerically calculate the expectation of a univariate function $v(x)$ with respect to a normal distribution.  \n",
    "2. How to apply the Gauss–Hermite quadrature method for this task.  \n",
    "3. Since these lecture notes focus on machine learning methods, we will emphasize the case where $v(x)$ is represented by a neural network.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509811e9-d4cb-4c0c-b849-1ea00d7c1954",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Consider a function $v: \\mathbb{R} \\to \\mathbb{R}$.  \n",
    "We are interested in numerically computing  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[v(X)] = \\int_{-\\infty}^{\\infty} v(x) f(x;\\mu,\\sigma) dx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ is the pdf of a normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3df722-22bb-4dd8-aaec-11edb286a23e",
   "metadata": {},
   "source": [
    "## Gauss–Hermite quadrature\n",
    "\n",
    "This method approximates the following integral by calculating a weighted sum of the function values at specific points, called **nodes**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int_{-\\infty}^{\\infty} h(z) e^{-z^2} \\, dz \\;\\approx\\; \\sum_{i=1}^n w_i \\, h(z_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $n$: the number of nodes  \n",
    "- $w_i$: the quadrature weights  \n",
    "- $z_i$: the nodes, which are the roots of the *physicists’ version* of the [Hermite polynomial](https://en.wikipedia.org/wiki/Hermite_polynomials) of degree $n$.  \n",
    "\n",
    "---\n",
    "\n",
    "**Important note:** Our original goal is to calculate  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} v(x)\\, e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\, dx,\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "which involves a normal distribution. The Gauss–Hermite formula above looks slightly different because it is tailored to integrals of the form  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int_{-\\infty}^{\\infty} h(z) e^{-z^2} dz.\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "We will reconcile the two expressions by applying an appropriate change of variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d341fb6-48c8-490f-a389-26408e9ccae6",
   "metadata": {},
   "source": [
    "## Going from $\\int_{-\\infty}^{\\infty} h(z) e^{-z^2} \\, dz$ to $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} v(x) e^{-\\tfrac{(x-\\mu)^2}{2\\sigma^2}} \\, dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89663e6a-e4ee-4b79-a743-45098b1e1995",
   "metadata": {},
   "source": [
    "Consider the change of variable  \n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sqrt{2}\\sigma}.\n",
    "$$  \n",
    "\n",
    "Then  \n",
    "$$\n",
    "x = \\mu + \\sqrt{2}\\sigma z, \\qquad dx = \\sqrt{2}\\sigma \\, dz.\n",
    "$$  \n",
    "\n",
    "With this substitution, we can write  \n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} v(x) \n",
    "e^{-\\tfrac{(x-\\mu)^2}{2\\sigma^2}} \\, dx\n",
    "= \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} \n",
    "v(\\mu + \\sqrt{2}\\sigma z) e^{-z^2} \\, dz.\n",
    "$$  \n",
    "\n",
    "Define  \n",
    "$$\n",
    "\\tilde{v}(z) \\equiv \\frac{1}{\\sqrt{\\pi}} v(\\mu + \\sqrt{2}\\sigma z).\n",
    "$$  \n",
    "\n",
    "Then  \n",
    "$$\n",
    "\\mathbb{E}[v(X)] \n",
    "= \\int_{-\\infty}^{\\infty} \\tilde{v}(z) e^{-z^2} \\, dz\n",
    "\\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\mu + \\sqrt{2}\\sigma z_i),\n",
    "$$  \n",
    "where the last equality follows from Gauss–Hermite quadrature.\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement this in Python:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813da255-0219-459d-8c44-ded94856ce76",
   "metadata": {},
   "source": [
    "# importing packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edab8260-92e4-4150-8801-ccfb2992b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193a435b-73dd-4d0c-ac05-5799033d01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ev(v,μ,σ,n):\n",
    "    #v: function, μ : mean, σ: stdev, n: number of nodes\n",
    "    nodes, weights = np.polynomial.hermite.hermgauss(n)\n",
    "    nodes_scaled = μ + (nodes*σ*np.sqrt(2))\n",
    "    weights_scaled = weights/np.sqrt(np.pi)\n",
    "    function_values = v(nodes_scaled)\n",
    "    approx_mean = np.sum(function_values*weights_scaled)\n",
    "    return approx_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b859b5-ac15-454c-b745-2122b1af1200",
   "metadata": {},
   "source": [
    "## Simple examples\n",
    "\n",
    "### Example 1: Linear functions\n",
    "\n",
    "Consider the function  \n",
    "$$\n",
    "v(x) = a x + b.\n",
    "$$  \n",
    "\n",
    "Suppose $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$. We want to compute the expectation $\\mathbb{E}[v(X)]$. \n",
    "\n",
    "The closed-form solution is  \n",
    "$$\n",
    "\\mathbb{E}[v(X)] = a \\mu + b.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ccdbcc-0ad1-4e6a-a830-b249970d247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_1(x):\n",
    "    a = 2\n",
    "    b = 1\n",
    "    return  a*x+b\n",
    "\n",
    "def true_mean_v_1(a,b,μ,σ):\n",
    "    true_mean = a*μ + b\n",
    "    print(\"True mean =\", true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935262a7-b479-4553-ac57-17e26ab313bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate mean = 3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximate mean =\", Ev(v = v_1, μ = 1, σ = 2, n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb91c16-d6f2-464d-a679-e0e5c862d30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean = 3\n"
     ]
    }
   ],
   "source": [
    "true_mean_v_1(a = 2,b = 1, μ = 1, σ = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbff25-27d9-4ba0-8b22-36c5c4dd2c2f",
   "metadata": {},
   "source": [
    "### Example 2: Quadratic Functions\n",
    "\n",
    "Consider the function  \n",
    "$$\n",
    "v(x) = a x^2 + bx + c\n",
    "$$  \n",
    "\n",
    "We want to compute the expectation $\\mathbb{E}[v(X)]$. \n",
    "\n",
    "The closed-form solution is  \n",
    "$$\n",
    "E[v(X)] = a(\\mu^2+\\sigma^2)+ b\\mu+c.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487770b2-da45-4761-95bf-0fe80e568340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_2(x):\n",
    "    a = 1.1\n",
    "    b = -0.1\n",
    "    c = 0.8\n",
    "    return (a*x**2)+(b*x)+c\n",
    "\n",
    "def true_mean_v_2(a, b, c, μ, σ):\n",
    "    true_mean =  a*(σ**2 + μ**2) + b*μ + c\n",
    "    print(\"True mean =\", true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf2cf3a-4bad-4fd2-ba73-47bcbc5308e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate mean = 1.0990000000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximate mean =\", Ev(v = v_2, μ = 0.2, σ = 0.5, n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934d044e-ee0f-4bd9-859c-58ebe42fd0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean = 1.0990000000000002\n"
     ]
    }
   ],
   "source": [
    "true_mean_v_2(a = 1.1, b = -0.1, c = 0.8, μ = 0.2, σ = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b1721-a36c-4183-9c40-95b3ccf6d2ba",
   "metadata": {},
   "source": [
    "### Example 3: A function with a kink  \n",
    "Let's try something **more challenging**. \n",
    "\n",
    "Consider the function $v(x) = \\max\\{0, x\\}$, with $\\mu = 0$. \n",
    "\n",
    "The closed-form solution is  \n",
    "$$\n",
    "E[v(X)] = \\frac{\\sigma}{\\sqrt{2\\pi}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9c572e-e8b7-4020-8f71-024482ed861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_3(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def true_mean_v_3(σ):\n",
    "    true_mean =  σ/np.sqrt(2*np.pi)\n",
    "    print(\"True mean =\", true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1512727-2797-424f-b886-bb777415750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate mean = 0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximate mean =\", Ev(v = v_3, μ = 0, σ = 2, n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b76874a5-c47b-4a57-afec-7c7c90dab44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean = 0.7978845608028654\n"
     ]
    }
   ],
   "source": [
    "true_mean_v_3(σ = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7571b-6bdf-47f3-b336-e8dabd368635",
   "metadata": {},
   "source": [
    "With nonsmooth functions, such as those with a kink, Gauss–Hermite quadrature performs poorly when using only a few nodes.\n",
    "\n",
    "In general, the performance of quadrature methods depends on the smoothness of the function. For more details, [Art Owen's lecture notes](https://artowen.su.domains/mc/Ch-quadrature.pdf)\n",
    "\n",
    "Let's increase the number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c1c25c-e2fb-4a23-89c7-0944b850e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate mean = 0.7989796104552142\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximate mean =\", Ev(v = v_3, μ = 0, σ = 2, n = 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab7cd9-52d7-446e-9f5e-1e36b2547db9",
   "metadata": {},
   "source": [
    "## Implementing it for a neural network\n",
    "From a theoretical perspective, there is nothing special about neural networks. They are just a *function*. So one should be able to do what I just did above very easily.\n",
    "\n",
    "The problem arises because of computational reasons. Usually the data points are in a batch, and you have to broadcast it ... fix it later\n",
    "\n",
    "** fix this**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113337f-170c-4be8-8ebd-cf3174840a1d",
   "metadata": {},
   "source": [
    "I have an application of this sort in mind. Consider the following example:  \n",
    "\n",
    "We want to compute the expectation  \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[v(\\vec{K}, x') \\mid x],\n",
    "$$  \n",
    "\n",
    "where $\\vec{K} \\in \\mathbb{R}^k$ denotes other inputs to $v$, which we treat as fixed.  \n",
    "\n",
    "$$\n",
    "x' = \\rho x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,\\sigma^2).\n",
    "$$  \n",
    "\n",
    "Therefore, conditional on \\(x\\), we have  \n",
    "\n",
    "$$\n",
    "x' \\mid x \\sim \\mathcal{N}(\\rho x, \\sigma^2).\n",
    "$$  \n",
    "\n",
    "For a a given $\\vec{K} \\in \\mathbb{R}^k$ and $x\\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[v(\\vec{K}, x') \\mid x] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}, \\rho x + \\sqrt{2}\\sigma z_i)\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61c722-5d66-44d1-942f-eba0c7a37b13",
   "metadata": {},
   "source": [
    "When training a neural network, a `batch` of size $B$ looks like this:\n",
    "\n",
    "$$\n",
    "\\text{batch} =\n",
    "\\begin{bmatrix}\n",
    "K_1^1 & K_2^1 & \\cdots & K_k^1 & x^1 \\\\\n",
    "K_1^2 & K_2^2 & \\cdots & K_k^2 & x^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "K_1^B & K_2^B & \\cdots & K_k^B & x^B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $K_i^j$ shows the $i$-th element in batch $j$. \n",
    "\n",
    "---\n",
    "**Example:** \n",
    "\n",
    "If you are interested in solving the stochastic version of the neo-classical growth model, see [Ljungqvist and Sargent](https://www.sfu.ca/~kkasa/Recursive_Macroeconomic_Theory_Ljungqvist_Sargent_2018.pdf), Chapter 12, Section 2. A batch in this case looks like:\n",
    "\n",
    "$$\n",
    "\\text{batch} =\n",
    "\\begin{bmatrix}\n",
    "k^1 & x^1 \\\\\n",
    "k^2 & x^2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "k^B & x^B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\{k^1, \\dots, k^B\\}$ is a subset of the capital grid, and $\\{x^1, \\dots, x^B\\}$ is a subset of the total factor productivity grid.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508ee0f-ae1c-480d-abc5-0cf421c2e5f5",
   "metadata": {},
   "source": [
    "The task ahead is to compute the expectation of $v$ for each point in the batch.  \n",
    "\n",
    "More specifically, we want to calculate\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}[v(\\vec{K}^1, x^{\\prime 1}) \\mid x^1] \\\\\n",
    "\\mathbb{E}[v(\\vec{K}^2, x^{\\prime 2}) \\mid x^2] \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{E}[v(\\vec{K}^B, x^{\\prime B}) \\mid x^B]\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^1, \\rho x^1 + \\sqrt{2}\\sigma z_i) \\\\\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^2, \\rho x^2 + \\sqrt{2}\\sigma z_i) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^B, \\rho x^B + \\sqrt{2}\\sigma z_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's do some matrix manipulation. Define  \n",
    "\n",
    "$$\n",
    "\\zeta_i^j \\equiv \\rho x^j + \\sqrt{2}\\sigma z_i,\n",
    "$$\n",
    "\n",
    "where $x^j$ represents the $x$ in batch $j$, and $z_i$ corresponds to node $i$. Then we can rewrite:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^1, \\rho x^1 + \\sqrt{2}\\sigma z_i) \\\\\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^2, \\rho x^2 + \\sqrt{2}\\sigma z_i) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\, v(\\vec{K}^B, \\rho x^B + \\sqrt{2}\\sigma z_i)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{\\sqrt{\\pi}}\n",
    "\\begin{bmatrix}\n",
    "v(\\vec{K}^1, \\zeta^1_1) & v(\\vec{K}^1, \\zeta^1_2) & \\cdots & v(\\vec{K}^1, \\zeta^1_n) \\\\\n",
    "v(\\vec{K}^2, \\zeta^2_1) & v(\\vec{K}^2, \\zeta^2_2) & \\cdots & v(\\vec{K}^2, \\zeta^2_n) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v(\\vec{K}^B, \\zeta^B_1) & v(\\vec{K}^B, \\zeta^B_2) & \\cdots & v(\\vec{K}^B, \\zeta^B_n)\n",
    "\\end{bmatrix}_{B\\times n}\n",
    "\\begin{bmatrix}\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "\\vdots\\\\\n",
    "w_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The new matrix above has dimensions $B \\times n$, but the total number of inputs is $B \\times (k+1) \\times n$. Why?  \n",
    "- $B$: number of batches  \n",
    "- $k+1$: dimensionality of a single batch  \n",
    "- $n$: number of nodes used for Gauss–Hermite quadrature\n",
    "\n",
    "So we have to resort to **tensors**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9200f9-9985-47a7-943f-e6b04aa7ae9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a449d0-31cb-4f7d-ae7d-977f6df9c456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
