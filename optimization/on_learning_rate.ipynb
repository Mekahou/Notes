{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52544200-924a-4827-872d-bf688c756a89",
   "metadata": {},
   "source": [
    "# Goal of this Lecture Note\n",
    "\n",
    "In these notes, we want to investigate the role of the *learning rate*, in particular its magnitude, in the gradient descent algorithm, starting with the simplest possible setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Consider the general formulation of gradient descent.  \n",
    "Let $\\beta \\in \\mathbb{R}^d$ be the parameters we want to estimate, and let $\\mathcal{L}(\\beta)$ be the objective (e.g., loss function) we want to minimize.  \n",
    "Gradient descent updates the parameters according to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{t+1} = \\hat{\\beta}_t - \\lambda \\nabla_{\\beta} \\mathcal{L}(\\hat{\\beta}_t),\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why the Magnitude of the Learning Rate Matters\n",
    "\n",
    "The learning rate controls the fundamental trade-off between **speed** and **stability**.\n",
    "\n",
    "The gradient provides the *direction* of steepest descent, but the learning rate determines the *distance* traveled along that direction at each step.\n",
    "\n",
    "- If **$\\lambda$** is *small*, each update is cautious. Convergence is stable but slow.\n",
    "- If **$\\lambda$** is *large but still below the stability threshold*, updates are aggressive and convergence is fast.\n",
    "- If **$\\lambda$** is *too large*, the algorithm overshoots the minimum, oscillates, and can eventually diverge.\n",
    "\n",
    "To study these behaviors precisely and transparently, we now turn to a simple model where everything can be computed explicitly: **linear regression with an intercept and slope**.\n",
    "\n",
    "---\n",
    "\n",
    "## A Simple Test Case: Linear Regression\n",
    "\n",
    "We consider the parameter vector\n",
    "\n",
    "$$\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta^{(0)} \\\\\n",
    "\\beta^{(1)}\n",
    "\\end{pmatrix}\n",
    "\\in \\mathbb{R}^2,\n",
    "$$\n",
    "\n",
    "and the (mean squared error) loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N \\big( y_i - \\beta^{(0)} - \\beta^{(1)} x_i \\big)^2,\n",
    "$$\n",
    "\n",
    "where the dataset is\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N.\n",
    "$$\n",
    "\n",
    "This setting is simple enough to allow us to derive the gradient, Hessian, and the exact dynamical system generated by gradient descent, making it ideal for studying the role of the learning rate.\n",
    "\n",
    "## Calculating the gradient\n",
    "\n",
    "Then the gradient of the loss is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\beta \\mathcal{L}(\\beta) = \n",
    "-2\\begin{pmatrix}\n",
    "\\bar{y}-\\beta^{(0)} - \\beta^{(1)}\\bar{x}\\\\\n",
    "\\overline{yx}- \\beta^{(0)} \\bar{x} - \\beta^{(1)} \\overline{x^2}\n",
    "\\end{pmatrix}\n",
    "\\end{align},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i, \\quad\n",
    "\\bar{y} = \\frac{1}{N}\\sum_{i=1}^N y_i, \\quad\n",
    "\\overline{x^2} = \\frac{1}{N}\\sum_{i=1}^N x_i^2, \\quad\n",
    "\\overline{xy} = \\frac{1}{N}\\sum_{i=1}^N x_i y_i.\n",
    "$$\n",
    "\n",
    "The dynamical system that describes the gradient descent is\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\beta}^{(0)}_{t+1}\\\\[4pt]\n",
    "\\hat{\\beta}^{(1)}_{t+1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\beta}^{(0)}_{t}\\\\[4pt]\n",
    "\\hat{\\beta}^{(1)}_{t}\n",
    "\\end{pmatrix}\n",
    "+ \\lambda\\times 2\n",
    "\\begin{pmatrix}\n",
    "\\bar{y}-\\hat{\\beta}^{(0)}_t - \\hat{\\beta}^{(1)}_t \\bar{x}\\\\[6pt]\n",
    "\\overline{xy}- \\hat{\\beta}^{(0)}_t \\bar{x} - \\hat{\\beta}^{(1)}_t \\overline{x^2}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "which simplifies to\n",
    "$$\n",
    "\\hat{\\beta}_{t+1} = (I-2\\lambda P)\\hat{\\beta}_t + 2\\lambda c\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "P \\equiv \\begin{bmatrix}\n",
    "1 & \\bar{x}\\\\\n",
    "\\bar{x} & \\overline{x^2}\n",
    "\\end{bmatrix}, \n",
    "~\\text{and}~\n",
    "c \\equiv \\begin{pmatrix}\n",
    "\\bar{y}\\\\\n",
    "\\overline{xy}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Fixed point\n",
    "This system has a unique fixed point that solves\n",
    "$$\n",
    "P \\beta^* = b ~ \\text{or}~ \\begin{bmatrix}\n",
    "1 & \\bar{x}\\\\\n",
    "\\bar{x} & \\overline{x^2}\n",
    "\\end{bmatrix} \\begin{pmatrix}\n",
    "\\beta^{(0)^*}\\\\\n",
    "\\beta^{(1)^*} \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\bar{y}\\\\\n",
    "\\overline{xy}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^* = P^{-1}c\n",
    "\\end{align}\n",
    "$$\n",
    "This is exactly the 2-D version of $(X'X)^{-1}X'Y$ that we learn in stats/econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f19b5a-12f5-4885-8328-97918bfde581",
   "metadata": {},
   "source": [
    "## The stability of the dynamical system \n",
    "Define \n",
    "$$\n",
    "\\hat{e}_t \\equiv \\hat{\\beta}_t-\\beta^*,\n",
    "$$\n",
    "which represents the deviation from the fixed point. The dynamics of this deviation are\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{e}_{t+1} = (I-2\\lambda P) \\hat{e}_t.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $\\mu_1$ and $\\mu_2$ be the eigenvalues of $P$, and let $\\nu_1$ and $\\nu_2$ be the corresponding eigenvectors.\n",
    "\n",
    "The solution can be written as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{e}_t = \\alpha_1 \\nu_1 (1-2\\lambda \\mu_1)^t + \\alpha_2 \\nu_2 (1-2\\lambda \\mu_2)^t \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha_1$ and $\\alpha_2$ are determined by the initial condition $e_0 = \\hat{\\beta}_0-\\beta^*$. \n",
    "\n",
    "### Convergence\n",
    "For gradient descent to converge, we need\n",
    "$$\n",
    "\\lim_{t\\rightarrow 0} \\hat{e}_t = 0 \\quad \\text{for all} \\quad \\hat{e}_0\n",
    "$$\n",
    "\n",
    "That means \n",
    "1. $|1-2\\lambda \\mu_1|<1$ $\\quad$ $\\rightarrow$ $\\quad$ $0<\\lambda<\\frac{1}{\\mu_1}$\n",
    "2. $|1-2\\lambda \\mu_2|<1$ $\\quad$ $\\rightarrow$ $\\quad$ $0<\\lambda<\\frac{1}{\\mu_2}$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boxed{0 < \\lambda < \\frac{1}{\\mu_{\\text{max}}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "-----\n",
    "Let’s test this in practice. Here I use `PyTorch`, but the gradient can also be defined manually to reproduce the same update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe2ebd1-512b-40a2-85e2-ac9300a55f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f72a0d16-31e3-4a2a-a623-45b35e1a4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data\n",
    "torch.manual_seed(1)\n",
    "N = 200\n",
    "true_b0 = 1.5\n",
    "true_b1 = -2.0\n",
    "\n",
    "x = torch.randn(N, 1)                   # shape (N, 1)\n",
    "eps = 0.1 * torch.randn(N, 1)\n",
    "y = true_b0 + true_b1 * x + eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52838597-3753-4a5d-ac2f-f51830a0e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forming P matrix\n",
    "x_bar = x.mean()\n",
    "x_sq_bar = (x**2).mean()\n",
    "\n",
    "P = torch.tensor([[1.0,x_bar],[x_bar,x_sq_bar]])\n",
    "\n",
    "eigenvalues = torch.linalg.eigvalsh(P)\n",
    "\n",
    "μ_max = eigenvalues.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4314c10a-32eb-4215-aedc-42f7e5614a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the linear function (neural net)\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.weight.fill_(0.0)   # sets w = 0\n",
    "    model.bias.fill_(0.0)     # sets b = 0\n",
    "\n",
    "\n",
    "## setting the optimizer\n",
    "learning_rate = (1/μ_max)*(1.01)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e47479ab-8b84-482b-ba02-00cbe2c6c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | loss = 888188.62500 | b0 = 316.228, b1 = -877.056\n",
      "step    1 | loss = 924072.00000 | b0 = -319.534, b1 = 890.536\n",
      "step    2 | loss = 961404.25000 | b0 = 328.943, b1 = -912.408\n",
      "step    3 | loss = 1000244.50000 | b0 = -332.503, b1 = 926.595\n",
      "step    4 | loss = 1040654.56250 | b0 = 342.172, b1 = -949.187\n",
      "step    5 | loss = 1082695.25000 | b0 = -345.996, b1 = 964.109\n",
      "step    6 | loss = 1126435.62500 | b0 = 355.935, b1 = -987.453\n",
      "step    7 | loss = 1171943.50000 | b0 = -360.035, b1 = 1003.140\n",
      "step    8 | loss = 1219289.25000 | b0 = 370.254, b1 = -1027.264\n",
      "step    9 | loss = 1268548.37500 | b0 = -374.641, b1 = 1043.748\n",
      "step   10 | loss = 1319796.75000 | b0 = 385.152, b1 = -1068.684\n",
      "step   11 | loss = 1373115.62500 | b0 = -389.836, b1 = 1085.996\n",
      "step   12 | loss = 1428590.25000 | b0 = 400.652, b1 = -1111.777\n",
      "step   13 | loss = 1486302.50000 | b0 = -405.646, b1 = 1129.950\n",
      "step   14 | loss = 1546347.87500 | b0 = 416.777, b1 = -1156.611\n",
      "step   15 | loss = 1608820.62500 | b0 = -422.094, b1 = 1175.681\n",
      "step   16 | loss = 1673815.87500 | b0 = 433.555, b1 = -1203.256\n",
      "step   17 | loss = 1741436.75000 | b0 = -439.207, b1 = 1223.259\n",
      "step   18 | loss = 1811789.62500 | b0 = 451.010, b1 = -1251.785\n",
      "step   19 | loss = 1884985.00000 | b0 = -457.011, b1 = 1272.760\n",
      "step   20 | loss = 1961138.50000 | b0 = 469.170, b1 = -1302.276\n",
      "step   21 | loss = 2040368.37500 | b0 = -475.534, b1 = 1324.260\n",
      "step   22 | loss = 2122799.25000 | b0 = 488.064, b1 = -1354.808\n",
      "step   23 | loss = 2208563.00000 | b0 = -494.806, b1 = 1377.844\n",
      "step   24 | loss = 2297791.75000 | b0 = 507.721, b1 = -1409.462\n",
      "step   25 | loss = 2390621.25000 | b0 = -514.857, b1 = 1433.590\n",
      "step   26 | loss = 2487203.75000 | b0 = 528.173, b1 = -1466.323\n",
      "step   27 | loss = 2587688.00000 | b0 = -535.718, b1 = 1491.590\n",
      "step   28 | loss = 2692235.00000 | b0 = 549.451, b1 = -1525.483\n",
      "step   29 | loss = 2801000.75000 | b0 = -557.421, b1 = 1551.932\n",
      "step   30 | loss = 2914163.00000 | b0 = 571.588, b1 = -1587.031\n",
      "step   31 | loss = 3031893.50000 | b0 = -580.001, b1 = 1614.710\n",
      "step   32 | loss = 3154382.50000 | b0 = 594.620, b1 = -1651.066\n",
      "step   33 | loss = 3281819.00000 | b0 = -603.494, b1 = 1680.026\n",
      "step   34 | loss = 3414403.50000 | b0 = 618.582, b1 = -1717.687\n",
      "step   35 | loss = 3552344.00000 | b0 = -627.935, b1 = 1747.980\n",
      "step   36 | loss = 3695861.50000 | b0 = 643.513, b1 = -1787.001\n",
      "step   37 | loss = 3845174.50000 | b0 = -653.364, b1 = 1818.681\n",
      "step   38 | loss = 4000522.00000 | b0 = 669.450, b1 = -1859.116\n",
      "step   39 | loss = 4162145.50000 | b0 = -679.821, b1 = 1892.237\n",
      "step   40 | loss = 4330295.00000 | b0 = 696.436, b1 = -1934.144\n",
      "step   41 | loss = 4505244.00000 | b0 = -707.346, b1 = 1968.766\n",
      "step   42 | loss = 4687259.50000 | b0 = 724.512, b1 = -2012.203\n",
      "step   43 | loss = 4876622.50000 | b0 = -735.983, b1 = 2048.385\n",
      "step   44 | loss = 5073635.00000 | b0 = 753.722, b1 = -2093.414\n",
      "step   45 | loss = 5278610.00000 | b0 = -765.777, b1 = 2131.219\n",
      "step   46 | loss = 5491858.50000 | b0 = 784.111, b1 = -2177.906\n",
      "step   47 | loss = 5713734.50000 | b0 = -796.775, b1 = 2217.404\n",
      "step   48 | loss = 5944574.50000 | b0 = 815.729, b1 = -2265.813\n",
      "step   49 | loss = 6184736.00000 | b0 = -829.026, b1 = 2307.069\n",
      "step   50 | loss = 6434602.00000 | b0 = 848.625, b1 = -2357.271\n",
      "step   51 | loss = 6694558.50000 | b0 = -862.578, b1 = 2400.355\n",
      "step   52 | loss = 6965017.50000 | b0 = 882.849, b1 = -2452.422\n",
      "step   53 | loss = 7246398.00000 | b0 = -897.487, b1 = 2497.408\n",
      "step   54 | loss = 7539149.00000 | b0 = 918.456, b1 = -2551.418\n",
      "step   55 | loss = 7843731.00000 | b0 = -933.806, b1 = 2598.385\n",
      "step   56 | loss = 8160615.50000 | b0 = 955.501, b1 = -2654.414\n",
      "step   57 | loss = 8490307.00000 | b0 = -971.592, b1 = 2703.442\n",
      "step   58 | loss = 8833320.00000 | b0 = 994.043, b1 = -2761.573\n",
      "step   59 | loss = 9190189.00000 | b0 = -1010.905, b1 = 2812.742\n",
      "step   60 | loss = 9561473.00000 | b0 = 1034.142, b1 = -2873.057\n",
      "step   61 | loss = 9947748.00000 | b0 = -1051.806, b1 = 2926.457\n",
      "step   62 | loss = 10349635.00000 | b0 = 1075.861, b1 = -2989.047\n",
      "step   63 | loss = 10767756.00000 | b0 = -1094.360, b1 = 3044.765\n",
      "step   64 | loss = 11202765.00000 | b0 = 1119.265, b1 = -3109.722\n",
      "step   65 | loss = 11655360.00000 | b0 = -1138.632, b1 = 3167.855\n",
      "step   66 | loss = 12126232.00000 | b0 = 1164.423, b1 = -3235.273\n",
      "step   67 | loss = 12616131.00000 | b0 = -1184.692, b1 = 3295.917\n",
      "step   68 | loss = 13125823.00000 | b0 = 1211.405, b1 = -3365.895\n",
      "step   69 | loss = 13656088.00000 | b0 = -1232.614, b1 = 3429.154\n",
      "step   70 | loss = 14207812.00000 | b0 = 1260.285, b1 = -3501.799\n",
      "step   71 | loss = 14781812.00000 | b0 = -1282.472, b1 = 3567.774\n",
      "step   72 | loss = 15379005.00000 | b0 = 1311.140, b1 = -3643.192\n",
      "step   73 | loss = 16000320.00000 | b0 = -1334.344, b1 = 3711.994\n",
      "step   74 | loss = 16646735.00000 | b0 = 1364.050, b1 = -3790.295\n",
      "step   75 | loss = 17319256.00000 | b0 = -1388.312, b1 = 3862.037\n",
      "step   76 | loss = 18018936.00000 | b0 = 1419.097, b1 = -3943.338\n",
      "step   77 | loss = 18746892.00000 | b0 = -1444.460, b1 = 4018.141\n",
      "step   78 | loss = 19504248.00000 | b0 = 1476.368, b1 = -4102.563\n",
      "step   79 | loss = 20292202.00000 | b0 = -1502.876, b1 = 4180.552\n",
      "step   80 | loss = 21111992.00000 | b0 = 1535.952, b1 = -4268.224\n",
      "step   81 | loss = 21964908.00000 | b0 = -1563.652, b1 = 4349.525\n",
      "step   82 | loss = 22852280.00000 | b0 = 1597.943, b1 = -4440.580\n",
      "step   83 | loss = 23775536.00000 | b0 = -1626.883, b1 = 4525.333\n",
      "step   84 | loss = 24736092.00000 | b0 = 1662.440, b1 = -4619.898\n",
      "step   85 | loss = 25735406.00000 | b0 = -1692.670, b1 = 4708.236\n",
      "step   86 | loss = 26775130.00000 | b0 = 1729.542, b1 = -4806.464\n",
      "step   87 | loss = 27856858.00000 | b0 = -1761.114, b1 = 4898.532\n",
      "step   88 | loss = 28982280.00000 | b0 = 1799.355, b1 = -5000.563\n",
      "step   89 | loss = 30153154.00000 | b0 = -1832.324, b1 = 5096.514\n",
      "step   90 | loss = 31371348.00000 | b0 = 1871.989, b1 = -5202.503\n",
      "step   91 | loss = 32638720.00000 | b0 = -1906.410, b1 = 5302.492\n",
      "step   92 | loss = 33957336.00000 | b0 = 1947.556, b1 = -5412.604\n",
      "step   93 | loss = 35329220.00000 | b0 = -1983.489, b1 = 5516.796\n",
      "step   94 | loss = 36756528.00000 | b0 = 2026.177, b1 = -5631.192\n",
      "step   95 | loss = 38241484.00000 | b0 = -2063.682, b1 = 5739.753\n",
      "step   96 | loss = 39786416.00000 | b0 = 2107.975, b1 = -5858.609\n",
      "step   97 | loss = 41393784.00000 | b0 = -2147.116, b1 = 5971.722\n",
      "step   98 | loss = 43066116.00000 | b0 = 2193.077, b1 = -6095.217\n",
      "step   99 | loss = 44805980.00000 | b0 = -2233.919, b1 = 6213.062\n"
     ]
    }
   ],
   "source": [
    "b0_list_diverge = []\n",
    "b1_list_diverge = []\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "for t in range(num_steps):\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Extract parameters\n",
    "    b0_hat = model.bias.item()\n",
    "    b1_hat = model.weight.item()\n",
    "\n",
    "    # Save them\n",
    "    b0_list_diverge.append(b0_hat)\n",
    "    b1_list_diverge.append(b1_hat)\n",
    "\n",
    "    # Optional printout\n",
    "    #print(f\"step {t:4d} | loss = {loss:.5f} | b0 = {b0_hat:.3f}, b1 = {b1_hat:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c4923-4004-4ef3-b121-71f8931410ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
