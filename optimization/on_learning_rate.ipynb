{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52544200-924a-4827-872d-bf688c756a89",
   "metadata": {},
   "source": [
    "# Goal of this Lecture Note\n",
    "\n",
    "In these notes, we want to investigate the role of the *learning rate*, in particular its magnitude, in the gradient descent algorithm, starting with the simplest possible setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Consider the general formulation of gradient descent.  \n",
    "Let $\\beta \\in \\mathbb{R}^d$ be the parameters we want to estimate, and let $\\mathcal{L}(\\beta)$ be the objective (e.g., loss function) we want to minimize.  \n",
    "Gradient descent updates the parameters according to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{t+1} = \\hat{\\beta}_t - \\lambda \\nabla_{\\beta} \\mathcal{L}(\\hat{\\beta}_t),\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why the Magnitude of the Learning Rate Matters\n",
    "\n",
    "The learning rate controls the fundamental trade-off between **speed** and **stability**.\n",
    "\n",
    "The gradient provides the *direction* of steepest descent, but the learning rate determines the *distance* traveled along that direction at each step.\n",
    "\n",
    "- If **$\\lambda$** is *small*, each update is cautious. Convergence is stable but slow.\n",
    "- If **$\\lambda$** is *large but still below the stability threshold*, updates are aggressive and convergence is fast.\n",
    "- If **$\\lambda$** is *too large*, the algorithm overshoots the minimum, oscillates, and can eventually diverge.\n",
    "\n",
    "To study these behaviors precisely and transparently, we now turn to a simple model where everything can be computed explicitly: **linear regression with an intercept and slope**.\n",
    "\n",
    "---\n",
    "\n",
    "## A Simple Test Case: Linear Regression\n",
    "\n",
    "We consider the parameter vector\n",
    "\n",
    "$$\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta^{(0)} \\\\\n",
    "\\beta^{(1)}\n",
    "\\end{pmatrix}\n",
    "\\in \\mathbb{R}^2,\n",
    "$$\n",
    "\n",
    "and the (mean squared error) loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N \\big( y_i - \\beta^{(0)} - \\beta^{(1)} x_i \\big)^2,\n",
    "$$\n",
    "\n",
    "where the dataset is\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N.\n",
    "$$\n",
    "\n",
    "This setting is simple enough to allow us to derive the gradient, Hessian, and the exact dynamical system generated by gradient descent, making it ideal for studying the role of the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f19b5a-12f5-4885-8328-97918bfde581",
   "metadata": {},
   "source": [
    "## Calculating the gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
